# -*- coding: utf-8 -*-
"""DLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18kNr2HCnLIjs0ofXQzqXOnS9Z5fy3klk
"""

# 1. Import Organization
# Better organization of imports by category
# Data manipulation and analysis
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from plotly.offline import init_notebook_mode, iplot

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard

#Laod audio and extract feature
import librosa
from scipy.stats import skew, kurtosis

# 2.Data Loading and Preprocessing
def load_and_preprocess_data(file_path):
    # Load data
    df = pd.read_csv(file_path)

    # Create label encoder for gender
    label_mapping = {'male': 1, 'female': 0}
    y = df['label'].map(label_mapping).values

    # Drop label column to standardize features
    X = df.drop('label', axis=1)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)  # Note: only transform, not fit_transform

    # Reshape for RNN
    X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
    X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

    return df, X_train_reshaped, X_test_reshaped, y_train, y_test,scaler

# 3. \\Model Architecture
def create_rnn_model(input_shape, learning_rate=0.001):
    model = Sequential([
        # First RNN layer with batch normalization
        SimpleRNN(64, activation='relu', return_sequences=True,
                 input_shape=input_shape),
        tf.keras.layers.BatchNormalization(),
        Dropout(0.3),

        # Second RNN layer
        SimpleRNN(32, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        Dropout(0.3),

        # Dense layers for classification
        Dense(16, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])

    # Use Adam optimizer with specified learning rate
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC()]
    )

    return model

# 4. Training with Callbacks
def train_model(model, X_train, y_train, X_test, y_test):
    # Define callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    model_checkpoint = ModelCheckpoint(
        'best_model.keras',  # Change the filepath to end with '.keras'
        monitor='val_accuracy',
        save_best_only=True
    )

    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_test, y_test),
        callbacks=[early_stopping, model_checkpoint],
        verbose=1
    )


   # Extract training loss history
    training_loss_history = history.history['loss']  # All loss values for each epoch
    training_accuracy_history = history.history['accuracy']

    return history, training_loss_history, training_accuracy_history

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 5. Model Evaluation
def evaluate_model(model, X_test, y_test):
    # Make predictions
    predictions = model.predict(X_test)
    predictions_classes = (predictions > 0.5).astype(int).flatten()

    # Evaluate on test set
    loss, accuracy, AUC = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Loss: {loss:.4f}")
    print(f"Test Accuracy: {accuracy:.2f}")

    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_test, predictions_classes, target_names=["Class 0", "Class 1"]))

    # Confusion matrix
    cm = confusion_matrix(y_test, predictions_classes)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title("Confusion Matrix")
    plt.show()

    return predictions, predictions_classes, loss, accuracy

import librosa

# Function to extract features from an audio file
def extract_features_from_audio(file_path, sr=22050):
    # Load the audio file
    y, _ = librosa.load(file_path, sr=sr)

    # Extract features
    meanfreq = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
    sd = np.std(y)
    median = np.median(y)
    Q25 = np.percentile(y, 25)
    Q75 = np.percentile(y, 75)
    IQR = Q75 - Q25
    skewness = skew(y)
    kurt_value = kurtosis(y)
    sp_ent = np.mean(librosa.feature.spectral_flatness(y=y))
    sfm = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))
    mode = np.argmax(np.bincount(y.astype(int)))  # Approximate mode
    centroid = meanfreq  # Already calculated above
    meanfun = np.mean(librosa.feature.zero_crossing_rate(y=y))
    minfun = np.min(librosa.feature.zero_crossing_rate(y=y))
    maxfun = np.max(librosa.feature.zero_crossing_rate(y=y))
    meandom = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))
    mindom = np.min(librosa.feature.spectral_rolloff(y=y, sr=sr))
    maxdom = np.max(librosa.feature.spectral_rolloff(y=y, sr=sr))
    dfrange = maxdom - mindom
    modindx = np.mean(librosa.amplitude_to_db(abs(librosa.stft(y))))

    # Combine all features into an array
    features = np.array([
        meanfreq, sd, median, Q25, Q75, IQR, skewness, kurt_value,
        sp_ent, sfm, mode, centroid, meanfun, minfun, maxfun,
        meandom, mindom, maxdom, dfrange, modindx
    ])

    return features

# 6. Usage
if __name__ == "__main__":
    # Load and preprocess data
    data, X_train, X_test, y_train, y_test,scaler = load_and_preprocess_data('/content/voice.csv')

data.head()

# get total samples
n_samples = len(data)
# get total male samples
n_male_samples = len(data[data['label'] == 'male'])
# get total female samples
n_female_samples = len(data[data['label'] == 'female'])
print("Total samples:", n_samples)
print("Total male samples:", n_male_samples)
print("Total female samples:", n_female_samples)

data.info() #gives you detailes about data types

 #identify relationships between numeric features in ower dataset by visualizing how strongly they are correlated
 numeric_columns = data.select_dtypes(include=[np.number])
 f, ax = plt.subplots(figsize=(25, 15))
sns.heatmap(numeric_columns.corr(), annot=True, linewidths=0.5, linecolor="red", fmt='.1f', ax=ax)
plt.show()

# Create and train model
    model = create_rnn_model(input_shape=(X_train.shape[1], 1))
    history, training_loss_history,training_accuracy = train_model(model, X_train, y_train, X_test, y_test)

print("The final training loss value:", training_loss_history[-1])

 # Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.legend()
plt.grid()
plt.show()



print("The final training accuracy value:", training_accuracy[-1])

# Plot training and testing accuracy
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Over Epochs')
plt.legend()
plt.grid()
plt.show()

# Evaluate model
predictions, predictions_classes,test_loss,test_accuracy= evaluate_model(model, X_test, y_test)
print(predictions_classes)

#let's see if there is an overfitting
difference = test_loss - training_loss_history[-1]
print(f"The difference between the Training loss and the TEST loss: {difference:.5f}")
#A small difference between training and test loss generally indicates that your model is performing well
# and generalizing effectively to unseen data.

model.save("results/model.h5")

# Load and preprocess the audio

audio_file_path = '/content/Record(2)) (online-audio-converter.com).mp3'  # Replace with your audio path
features = extract_features_from_audio(audio_file_path)
print(features)
# Standardize features (use the scaler from training)
features_scaled = scaler.transform([features])

# Reshape for RNN input (1 sample, number of features, 1 channel)
features_reshaped = features_scaled.reshape((1, features_scaled.shape[1], 1))

# Predict using the trained model
prediction = model.predict(features_reshaped)
predicted_class = 'male' if prediction > 0.5 else 'female'
print(f"Predicted Gender: {predicted_class}")